# Task IV: Deployment Strategies
Models can be deployed in a number of ways, for example as a REST API web service, as a real-time streaming service, or to edge devices. 

Depending on the context, each of these may be appropriate. For example, if the goal of this is to create a heartbeat Arrythmia diagnosis application, a web server may be the most suitable as it can easily be accessed by external applications (it is also the easiest to deploy). However if this is for monitoring patients and Arrythmia poses an immediate risk, either a real-time streaming service or edge device would be more appropriate. If the model needs to be deployed along with specific hardware for measuring heartbeats, and may need to be mobile or be used in emergency situations where network connectivity is not gaurenteed, then an edge device that can function independently may be the most appropriate.

A few important considerations which can help to decide the deployment strategy are:
- Who needs to access the service (is it public, or for internal use)
- How quickly do results need to be returned (e.g. real time, of is a delay fine)
- How much bandwidth/compute is needed? (if it needs a lot, it may limit what devices it can be deployed on)

Separate from these considerations, we also need to ensure the model is working as expected. In some domains, data can drift causing the model to become less accurate over time. Also, if we are constantly improving the model and releasing new versions there is always the chance of a bad release where a model performs much worse in the real world compared to the test environment. There are a few strategies to help mitigate these issues that should incorporated into automatic pipelines:
- monitor incoming data and outgoing predictions with statistical analysis to ensure they belong to the same distributions as historical data. Any differences should be reported. This may also help prevent a wide range of issues such as faulty connections, or identify situations where the model may not be adequately trained
- when releasing models, have multiple-validation checks. For example, before moving a model to production, a final holdout set should be set aside to pass/fail the model during staging.
- infravalidation can also be used before pushing models to production in order to verify that they work as expected in the test environment. 


A possible scalable deployment environment might look like this:
1. heart beats ECG signals are collected and streamed using Apache Kafka to an Apache Spark cluster
2. a load manager directs the stream to an available server, where the data is processed
3. the data stream is then split to perform feature extraction and run validation statistics 
4. the feature-extracted stream is then used to run inference on
5. the inference results and statistics are then joined and streamed back over Kafka to subscribed users (e.g. nursing stations, other web services) and to a data sink (e.g. a SQL database).

When deploying a new model, the pipelines may look like
1. Perform hyperparameter optimization with the model (to prevent overfitting use a validation set for callbacks such as early stopping)
2. Select the hyperparameters based on a models performance on a test dataset
3. Evaluate the model on a holdout set to see if it outperforms the current production model by a sufficient amount
5. Move the model to staging and run infravalidation on the model in a production environment (evaluate with another holdout dataset) to see how the model functions in the real world
6. Deploy the model to production 