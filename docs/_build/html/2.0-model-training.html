<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Task II: Model Training and Fine Tuning &#8212; telfer-ecg-heartbeat-categorization-task 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/classic.css" />
    
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Task III: Testing the Holdout Set" href="3.0-holdout-testing.html" />
    <link rel="prev" title="Task I: Data Processing" href="1.0-data-processing.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="3.0-holdout-testing.html" title="Task III: Testing the Holdout Set"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="1.0-data-processing.html" title="Task I: Data Processing"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">telfer-ecg-heartbeat-categorization-task 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Task II: Model Training and Fine Tuning</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="task-ii-model-training-and-fine-tuning">
<h1>Task II: Model Training and Fine Tuning<a class="headerlink" href="#task-ii-model-training-and-fine-tuning" title="Permalink to this heading">¶</a></h1>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this heading">¶</a></h2>
<p>Code: https://github.com/A-Telfer/telfer-ecg-heartbeat-categorization-task/blob/master/src/models/train_model.py</p>
<p>We implemented a simple Model using stacked linear layers (size=2048) in Pytorch and Lightning. Despite it’s simplicity, from previous experience I know Linear models have worked well with real-world EEG datasets which similar properties. Linear models are also often faster than convolution models (can depend on parameters such as stride), though they are much larger.</p>
<p>An alternative approach may have been to use a 1D Convolutional model. One of the main strengths of convolutional models is their ability to extract spatial and frequency information, however we have already moved towards this by using wavelet features. Convolutional models are often slower and can be harder to restructure when changing the feature space.</p>
<p>Another approach may have been to use an LSTM or other recurrent model, however these are considerably slower to train/infer with and can struggle with longer signals (similarly attention/transformer networks can struggle with longer signals).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinearModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">input_size</span><span class="o">=</span><span class="mi">187</span><span class="p">,</span>
        <span class="n">output_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
        <span class="n">num_hidden_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">hidden_layer_size</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>

        <span class="n">hidden_layers</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_hidden_layers</span><span class="p">):</span>
            <span class="n">hidden_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="n">hidden_layer_size</span><span class="p">))</span>
            <span class="n">hidden_layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">())</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(),</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_layer_size</span><span class="p">),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
            <span class="o">*</span><span class="n">hidden_layers</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">LeakyReLU</span><span class="p">(),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_layer_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="o">...</span>
</pre></div>
</div>
</section>
<section id="hyper-parameter-optimization">
<h2>Hyper Parameter Optimization<a class="headerlink" href="#hyper-parameter-optimization" title="Permalink to this heading">¶</a></h2>
<p>Code: https://github.com/A-Telfer/telfer-ecg-heartbeat-categorization-task/blob/master/src/models/hparam_opt.py</p>
<p>We performed hyper-parameter optimization on the following parameters simultaneously using <code class="docutils literal notranslate"><span class="pre">optuna</span></code> and <code class="docutils literal notranslate"><span class="pre">mlflow</span></code>:</p>
<ul class="simple">
<li><p>Learning Rate:</p>
<ul>
<li><p>Range: [1e-1, 1e-5]</p></li>
<li><p>Sampling: Log uniform</p></li>
</ul>
</li>
<li><p>Momentum:</p>
<ul>
<li><p>Values: [0, 0.99]</p></li>
<li><p>Sampling: Uniform</p></li>
</ul>
</li>
<li><p>Weight Decay</p>
<ul>
<li><p>Values: [0, 1e-2]</p></li>
<li><p>Sampling: Log uniform</p></li>
</ul>
</li>
<li><p>Model Hidden Layers:</p>
<ul>
<li><p>Values: {1,2,3,4}</p></li>
<li><p>Sampling: Categorical</p></li>
</ul>
</li>
</ul>
<p>Optimization was performed using Tree of Parzen Estimators (TPE) implemented in the <code class="docutils literal notranslate"><span class="pre">optuna</span></code> package. For demonstrative purposes, we only performed 20 runs. Each run consisted of up to 100 training epochs (shorter depending on the early-stopping callback).</p>
<p>The training dataset was balanced during the augmentation stage, so we optimized using a standard unweighted crossentropy loss. The validation, test, and holdout datasets were not balanced however. Therefore to evaluate models we used the AUROC (with a macro average such that class was given equal weighting).</p>
<p>The optimal parameters that were found after 20 runs were: <code class="docutils literal notranslate"><span class="pre">learning_rate=0.0053</span></code>, <code class="docutils literal notranslate"><span class="pre">num_layers=4</span></code>, <code class="docutils literal notranslate"><span class="pre">momentum=0.969</span></code>, <code class="docutils literal notranslate"><span class="pre">weight_decay=0.0001</span></code></p>
<p>Given that we were optimizing several values at once, 20 runs is unlikely to be enough. An improvement could have been to optimize one parameter at a time, which pytorch has previously used for achieving state-of-the-art performance on imagenet: https://pytorch.org/blog/how-to-train-state-of-the-art-models-using-torchvision-latest-primitives/)</p>
</section>
<section id="overfitting-and-underfitting">
<h2>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this heading">¶</a></h2>
<p>To prevent overfitting we used a weight decay, dropout layers after each hidden layer, and implemented our own early stopping callback with checkpointing.</p>
<p>Including the model-size (number of hidden layers) in the hyper-parameter optimization also served to help avoid overfitting/underfitting as increasing the number of parameters in the model can help to prevent underfitting, and decreasing the number of parameters can help to prevent overfitting.</p>
</section>
<section id="early-stopping-callback">
<h2>Early Stopping Callback<a class="headerlink" href="#early-stopping-callback" title="Permalink to this heading">¶</a></h2>
<p>We implemented early stopping callback that stops the training and loads the best model. The rationale behind early stopping is simple, once the model stops improving on the validation set then the model is likely beginning to overfit to the training dataset. We add <code class="docutils literal notranslate"><span class="pre">patience</span></code> to allow for the model to exit suboptimal minimas that cause the validation metric to worsen temporarily.</p>
<p><img alt="" src="_images/early_stopping_by_auc.png" /></p>
<p><em>Figure 4: Evaluation runs show early stopping after validation AUC plateaus.</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lass</span> <span class="n">MetricsCallback</span><span class="p">(</span><span class="n">Callback</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mlflow_run</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">early_stopping_patience</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>

        <span class="c1"># Define metrics</span>
        <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">auroc_macro</span> <span class="o">=</span> <span class="n">torchmetrics</span><span class="o">.</span><span class="n">AUROC</span><span class="p">(</span>
            <span class="n">task</span><span class="o">=</span><span class="s2">&quot;multiclass&quot;</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s2">&quot;macro&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Early stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stop_on_next_train_epoch_end</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_last_value</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_patience</span> <span class="o">=</span> <span class="n">early_stopping_patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_count</span> <span class="o">=</span> <span class="n">early_stopping_patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_checkpoint</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mlflow_run</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">run_id</span><span class="si">}</span><span class="s2">_best.pt&quot;</span>

    <span class="o">...</span>

    <span class="k">def</span> <span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="o">...</span>

        <span class="c1"># Early stopping</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">stop_on_next_train_epoch_end</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">should_stop</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">pl_module</span><span class="o">.</span><span class="n">_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_checkpoint</span><span class="p">)</span>
            <span class="n">os</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_checkpoint</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="s2">&quot;early stopping triggered, returning best checkpoint&quot;</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">on_validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">pl_module</span><span class="p">):</span>
        <span class="n">auroc</span> <span class="o">=</span> <span class="o">...</span>

        <span class="c1"># Early stopping</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_last_value</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="ow">or</span> <span class="n">auroc</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_last_value</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_last_value</span> <span class="o">=</span> <span class="n">auroc</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_count</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_patience</span>

            <span class="c1"># Save best checkpoint</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">pl_module</span><span class="o">.</span><span class="n">_model</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_checkpoint</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_count</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping_count</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stop_on_next_train_epoch_end</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
</section>
<section id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this heading">¶</a></h2>
<p>Code: https://github.com/A-Telfer/telfer-ecg-heartbeat-categorization-task/blob/master/notebooks/4.0-telfer-explore-results.ipynb</p>
<p>We used the validation dataset for early stopping, and the testing dataset was used to select a model for hyper-parameter optimization, so evaluation was run on a separate holdout set. (Performance was higher on the testing dataset as expected and is compared in the next section)</p>
<p>On the holdout set, the model had an AUROC of 0.9833. It’s average accuracy across classes was 0.8961, however the micro accuracy on all test cases (not accounting for class imbalance) was 0.9445 due to its high performance on the over-represented classes.</p>
<p><img alt="" src="_images/confusion_matrix_holdout.png" /></p>
<p><em>Figure 5: Confusion Matrix results on the holdout dataset normalized over the True label. We see that the model struggles the most to classify Class 1, often confusing it for Class 0.</em></p>
<p>There are many ways we may improve the model, here are a few examples</p>
<ul class="simple">
<li><p>The model may be overly simplistic. Even a basic convolutional neural networks may provide better performance as they are well suited to tasks when data is spatially or temporally related.</p></li>
<li><p>The learning rate can be decreased overtime, either using a learning rate callback or a learning rate scheduler.</p></li>
<li><p>The number of runs for hyper-parameter optimization was very small compared to the number of parameters being optimized. Increasing the runs, or optimizing fields individually is likely to improve results.</p></li>
<li><p>The hyper-parameter optimization can be expanded to compare different features, augmentation approaches, and model types.</p></li>
<li><p>I think contrastive learning (perhaps with Triplet-loss) may provide interesting features and improve performance</p></li>
</ul>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Task II: Model Training and Fine Tuning</a><ul>
<li><a class="reference internal" href="#model">Model</a></li>
<li><a class="reference internal" href="#hyper-parameter-optimization">Hyper Parameter Optimization</a></li>
<li><a class="reference internal" href="#overfitting-and-underfitting">Overfitting and Underfitting</a></li>
<li><a class="reference internal" href="#early-stopping-callback">Early Stopping Callback</a></li>
<li><a class="reference internal" href="#evaluation">Evaluation</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="1.0-data-processing.html"
                          title="previous chapter">Task I: Data Processing</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="3.0-holdout-testing.html"
                          title="next chapter">Task III: Testing the Holdout Set</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/2.0-model-training.md.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="3.0-holdout-testing.html" title="Task III: Testing the Holdout Set"
             >next</a> |</li>
        <li class="right" >
          <a href="1.0-data-processing.html" title="Task I: Data Processing"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">telfer-ecg-heartbeat-categorization-task 0.1 documentation</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Task II: Model Training and Fine Tuning</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright .
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 7.0.1.
    </div>
  </body>
</html>